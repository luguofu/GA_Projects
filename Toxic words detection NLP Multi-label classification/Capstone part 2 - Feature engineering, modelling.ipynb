{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f2b099",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 45px\">\n",
    "\n",
    "# Capstone Project: Multi-Label Text classification for Trust and Safety (Content moderation) - Part 2 (Feature Extraction, Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6618f8",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "571bf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# module for tokenizer, stemming and lemmatization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer      \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# feature extraction / Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "#Evalutation\n",
    "from surprise import SVD\n",
    "#from surprise.model_selection import cross_validate, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise import accuracy #pip install scikit-surprise\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b81573",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f060e",
   "metadata": {},
   "source": [
    "#### Part 2:\n",
    "- Feature Extraction\n",
    "- Train Test Split\n",
    "- Baseline Modelling\n",
    "- Model Optimization through GridSearch CV\n",
    "- Model Analysis\n",
    "- Conclusion and Recommedations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803ec2f",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a0939b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_pickle('./datasets/df.pkl')\n",
    "# make sure to run the save to pickle file from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4735cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>senti_scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment_type</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stems</th>\n",
       "      <th>stop_snow_stems</th>\n",
       "      <th>lems</th>\n",
       "      <th>stop_lems</th>\n",
       "      <th>...</th>\n",
       "      <th>stop_lems_wcount</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>comment_wordcount</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>safe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>why the edits made under my username hardcore ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'comp...</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>[why, the, edits, made, under, my, username, h...</td>\n",
       "      <td>[whi, the, edit, made, under, my, usernam, har...</td>\n",
       "      <td>[whi, edit, made, usernam, hardcor, metallica,...</td>\n",
       "      <td>[why, the, edits, made, under, my, username, h...</td>\n",
       "      <td>[edits, made, username, hardcore, metallica, f...</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>230</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>{'neg': 0.118, 'neu': 0.71, 'pos': 0.172, 'com...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "      <td>[daww, he, match, this, background, colour, im...</td>\n",
       "      <td>[daww, match, background, colour, im, seem, st...</td>\n",
       "      <td>[daww, he, match, this, background, colour, im...</td>\n",
       "      <td>[daww, match, background, colour, im, seemingl...</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>{'neg': 0.083, 'neu': 0.849, 'pos': 0.068, 'co...</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "      <td>[hey, man, im, realli, not, tri, to, edit, war...</td>\n",
       "      <td>[hey, man, im, realli, tri, edit, war, guy, co...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>227</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>i cant make any real suggestions on improvemen...</td>\n",
       "      <td>{'neg': 0.044, 'neu': 0.893, 'pos': 0.063, 'co...</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>[i, cant, make, any, real, suggestions, on, im...</td>\n",
       "      <td>[i, cant, make, ani, real, suggest, on, improv...</td>\n",
       "      <td>[cant, make, ani, real, suggest, improv, wonde...</td>\n",
       "      <td>[i, cant, make, any, real, suggestion, on, imp...</td>\n",
       "      <td>[cant, make, real, suggestion, improvement, wo...</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>593</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'comp...</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[you, sir, are, my, hero, ani, chanc, you, rem...</td>\n",
       "      <td>[sir, hero, ani, chanc, rememb, page]</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[sir, hero, chance, remember, page, thats]</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  0000997932d777bf  why the edits made under my username hardcore ...   \n",
       "1  000103f0d9cfb60f  daww he matches this background colour im seem...   \n",
       "2  000113f07ec002fd  hey man im really not trying to edit war its j...   \n",
       "3  0001b41b1c6bb37e  i cant make any real suggestions on improvemen...   \n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                        senti_scores  compound sentiment_type  \\\n",
       "0  {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'comp...    0.5574       POSITIVE   \n",
       "1  {'neg': 0.118, 'neu': 0.71, 'pos': 0.172, 'com...    0.2263        NEUTRAL   \n",
       "2  {'neg': 0.083, 'neu': 0.849, 'pos': 0.068, 'co...   -0.1779        NEUTRAL   \n",
       "3  {'neg': 0.044, 'neu': 0.893, 'pos': 0.063, 'co...    0.2500       POSITIVE   \n",
       "4  {'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'comp...    0.6808       POSITIVE   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [why, the, edits, made, under, my, username, h...   \n",
       "1  [daww, he, matches, this, background, colour, ...   \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...   \n",
       "3  [i, cant, make, any, real, suggestions, on, im...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                      snowball_stems  \\\n",
       "0  [whi, the, edit, made, under, my, usernam, har...   \n",
       "1  [daww, he, match, this, background, colour, im...   \n",
       "2  [hey, man, im, realli, not, tri, to, edit, war...   \n",
       "3  [i, cant, make, ani, real, suggest, on, improv...   \n",
       "4  [you, sir, are, my, hero, ani, chanc, you, rem...   \n",
       "\n",
       "                                     stop_snow_stems  \\\n",
       "0  [whi, edit, made, usernam, hardcor, metallica,...   \n",
       "1  [daww, match, background, colour, im, seem, st...   \n",
       "2  [hey, man, im, realli, tri, edit, war, guy, co...   \n",
       "3  [cant, make, ani, real, suggest, improv, wonde...   \n",
       "4              [sir, hero, ani, chanc, rememb, page]   \n",
       "\n",
       "                                                lems  \\\n",
       "0  [why, the, edits, made, under, my, username, h...   \n",
       "1  [daww, he, match, this, background, colour, im...   \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...   \n",
       "3  [i, cant, make, any, real, suggestion, on, imp...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                           stop_lems  ...  stop_lems_wcount  \\\n",
       "0  [edits, made, username, hardcore, metallica, f...  ...                25   \n",
       "1  [daww, match, background, colour, im, seemingl...  ...                11   \n",
       "2  [hey, man, im, really, trying, edit, war, guy,...  ...                22   \n",
       "3  [cant, make, real, suggestion, improvement, wo...  ...                50   \n",
       "4         [sir, hero, chance, remember, page, thats]  ...                 6   \n",
       "\n",
       "   comment_length  comment_wordcount  toxic  severe_toxic  obscene  threat  \\\n",
       "0             230                 41      0             0        0       0   \n",
       "1              90                 14      0             0        0       0   \n",
       "2             227                 42      0             0        0       0   \n",
       "3             593                107      0             0        0       0   \n",
       "4              62                 13      0             0        0       0   \n",
       "\n",
       "   insult  identity_hate  safe  \n",
       "0       0              0     1  \n",
       "1       0              0     1  \n",
       "2       0              0     1  \n",
       "3       0              0     1  \n",
       "4       0              0     1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51d1f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.iloc[:, 14::].columns # y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a340f628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
       "       'safe'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories # y target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72519b",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa1253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer and lemmatizer function for TF-ID vectorizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    lem = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c55d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate TF-IDF\n",
    "tvec = TfidfVectorizer(stop_words='english', tokenizer=tokenize_and_lemmatize, ngram_range=(1, 2), max_features=8000,) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96882cb0",
   "metadata": {},
   "source": [
    "## 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c731114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Fit transform TF-IDF vectorizer on comment text col\n",
    "\n",
    "X = tvec.fit_transform(df['comment_text']) #Features\n",
    "y = df[categories] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88052fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (159571, 8000)\n",
      "y :  (159571, 7)\n"
     ]
    }
   ],
   "source": [
    "#check shape \n",
    "print(f'X : {X.shape}')\n",
    "print(f'y :  {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c748d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "277c9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (159571, 8000)\n",
      "y :  (159571, 7)\n"
     ]
    }
   ],
   "source": [
    "#check shape\n",
    "print(f'X : {X.shape}')\n",
    "print(f'y :  {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d9d8e",
   "metadata": {},
   "source": [
    "## 3. Baseline Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad3056",
   "metadata": {},
   "source": [
    "- Generate Baseline Models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "95f27948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up baseline model performance table\n",
    "model_performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ab8e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for model metric scores\n",
    "   \n",
    "def get_performance(y_test, y_pred):\n",
    "    # Evaluate Performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Get precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = score(y_test, y_pred, average='micro') \n",
    "    #Micro-average will aggregate the contributions of all classes to compute the average metric\n",
    "    \n",
    "    return accuracy, precision, recall, f1score\n",
    "\n",
    "#In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd55f10",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a64c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Instantiate lr model and fit \n",
    "lr = LogisticRegression()\n",
    "oneVsRes_lr = OneVsRestClassifier(lr) # OVR a method for using binary classification algorithms for multi-label classification problems\n",
    "oneVsRes_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7fb2166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Logistic Reg.: 0.9154989597172436\n",
      "Precision : 0.9463262238090764\n",
      "Recall    : 0.9018602672875019\n",
      "F1-score   : 0.9235583370585606\n"
     ]
    }
   ],
   "source": [
    "y_pred = oneVsRes_lr.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Logistic Reg.: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7774db",
   "metadata": {},
   "source": [
    "- Test scores performed too well. This i assessed, is due to data imbalance that we have identified during the EDA Process. logistic regression tends to have a bias prediction based on the majority of the class data.\n",
    "\n",
    "- We will treat the Data imbalance with setting the hyperparameter in the model, class weight = 'balance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e47a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(class_weight='balanced'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Instantiate lr model and fit \n",
    "lr = LogisticRegression(class_weight = 'balanced')\n",
    "oneVsRes_lr = OneVsRestClassifier(lr) # OVR a method for using binary classification algorithms for multi-class classification problems\n",
    "oneVsRes_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9aa0d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Logistic Regression: 0.8543102799989973\n",
      "Precision : 0.819183343440469\n",
      "Recall    : 0.9071881086163283\n",
      "F1-score   : 0.860942628610276\n",
      "Train_score : 0.8563144437574157\n",
      "Test_score : 0.8543102799989973\n"
     ]
    }
   ],
   "source": [
    "y_pred = oneVsRes_lr.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Logistic Regression: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "print(f'Train_score : {oneVsRes_lr.score(X_train, y_train)}')\n",
    "print(f'Test_score : {oneVsRes_lr.score(X_test, y_test)}')\n",
    "\n",
    "# Add performance parameters to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'Logistic Regression'),\n",
    "    ('Test Accuracy', accuracy),\n",
    "    ('Precision', precision),\n",
    "    ('Recall', recall),\n",
    "    ('F1', f1score),\n",
    "    ('Train Score', oneVsRes_lr.score(X_train, y_train)),\n",
    "    ('Test Score', oneVsRes_lr.score(X_test, y_test))\n",
    "    \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b20dc1",
   "metadata": {},
   "source": [
    "- After setting the class weight to balanced, the scores declined. This proves that data imbalance was indeed affecting the scores with bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e60c2b",
   "metadata": {},
   "source": [
    "### Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c4c1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_jobs=-1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(class_weight = 'balanced', n_jobs = -1) # setting class_weight to balance to treat data imbalance\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c41c20ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Random Forrest Classifier: 0.8982528263103803\n",
      "Precision : 0.8982528263103803\n",
      "Recall    : 0.802175908307403\n",
      "F1-score   : 0.8475001182536304\n",
      "Train_score : 0.898343889436655\n",
      "Test_score : 0.8982528263103803\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Random Forrest Classifier: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "print(f'Train_score : {rf.score(X_train, y_train)}')\n",
    "print(f'Test_score : {rf.score(X_test, y_test)}')\n",
    "\n",
    "# Add performance parameters to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'Random Forrest Classifier'),\n",
    "    ('Test Accuracy', accuracy),\n",
    "    ('Precision', precision),\n",
    "    ('Recall', recall),\n",
    "    ('F1', f1score),\n",
    "    ('Train Score', rf.score(X_train, y_train)),\n",
    "    ('Test Score', rf.score(X_test, y_test))\n",
    "    \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202ee14",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8884f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c4ed7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Decision Tree Classifier: 0.7639686160479282\n",
      "Precision : 0.6378176518808635\n",
      "Recall    : 0.7837970943117458\n",
      "F1-score   : 0.7033123757105838\n",
      "Train_score : 0.9853523621718278\n",
      "Test_score : 0.7639686160479282\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Decision Tree Classifier: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "print(f'Train_score : {clf.score(X_train, y_train)}')\n",
    "print(f'Test_score : {clf.score(X_test, y_test)}')\n",
    "\n",
    "# Add performance parameters to list\n",
    "model_performance.append(dict([\n",
    "    ('Model', 'Decision Tree Classifier'),\n",
    "    ('Test Accuracy', accuracy),\n",
    "    ('Precision', precision),\n",
    "    ('Recall', recall),\n",
    "    ('F1', f1score),\n",
    "    ('Train Score', clf.score(X_train, y_train)),\n",
    "    ('Test Score', clf.score(X_test, y_test))\n",
    "    \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd493a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'Logistic Regression',\n",
       "  'Test Accuracy': 0.8543102799989973,\n",
       "  'Precision': 0.819183343440469,\n",
       "  'Recall': 0.9071881086163283,\n",
       "  'F1': 0.860942628610276,\n",
       "  'Train Score': 0.8563144437574157,\n",
       "  'Test Score': 0.8543102799989973},\n",
       " {'Model': 'Random Forrest Classifier',\n",
       "  'Test Accuracy': 0.8982528263103803,\n",
       "  'Precision': 0.8982528263103803,\n",
       "  'Recall': 0.802175908307403,\n",
       "  'F1': 0.8475001182536304,\n",
       "  'Train Score': 0.898343889436655,\n",
       "  'Test Score': 0.8982528263103803},\n",
       " {'Model': 'Decision Tree Classifier',\n",
       "  'Test Accuracy': 0.7639686160479282,\n",
       "  'Precision': 0.6378176518808635,\n",
       "  'Recall': 0.7837970943117458,\n",
       "  'F1': 0.7033123757105838,\n",
       "  'Train Score': 0.9853523621718278,\n",
       "  'Test Score': 0.7639686160479282}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4831b4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.854310</td>\n",
       "      <td>0.819183</td>\n",
       "      <td>0.907188</td>\n",
       "      <td>0.860943</td>\n",
       "      <td>0.856314</td>\n",
       "      <td>0.854310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forrest Classifier</th>\n",
       "      <td>0.898253</td>\n",
       "      <td>0.898253</td>\n",
       "      <td>0.802176</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.898344</td>\n",
       "      <td>0.898253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.763969</td>\n",
       "      <td>0.637818</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>0.763969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Test Accuracy  Precision    Recall        F1  \\\n",
       "Model                                                                     \n",
       "Logistic Regression             0.854310   0.819183  0.907188  0.860943   \n",
       "Random Forrest Classifier       0.898253   0.898253  0.802176  0.847500   \n",
       "Decision Tree Classifier        0.763969   0.637818  0.783797  0.703312   \n",
       "\n",
       "                           Train Score  Test Score  \n",
       "Model                                               \n",
       "Logistic Regression           0.856314    0.854310  \n",
       "Random Forrest Classifier     0.898344    0.898253  \n",
       "Decision Tree Classifier      0.985352    0.763969  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate baseline score summary table\n",
    "results = pd.DataFrame(data=model_performance)\n",
    "results = results[['Model', 'Test Accuracy', 'Precision', 'Recall', 'F1', 'Train Score', 'Test Score']]\n",
    "results = results.sort_values(by='F1', ascending=False)\n",
    "results = results.set_index('Model')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c8ef0",
   "metadata": {},
   "source": [
    "- Based on F1 scores, Random Forrest Classifier performed the best while Logestic Regression performed slightly poorer. \n",
    "- Train and Test score indicates that Random Forrest Classifier is overfitted whilst Logistic Regression is not.\n",
    "- With the baseline scores generated, we will now utilise GridsearchCV to optimize our selected 2 models, Logistic Regression and Random Forrest Classifier with hypertuninng parameters for better performing results.\n",
    "- we will ommit out Decision Tree classifier as it performed the poorest among all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d34c7",
   "metadata": {},
   "source": [
    "## 5. Model Optimization through GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ae61df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up baseline model performance table\n",
    "model_performance_tuned = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9497d3",
   "metadata": {},
   "source": [
    "### GridSearch Radomn Forrest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "735b4dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd790882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate parameters for Gridsearch\n",
    "param_gs = {\n",
    "    'max_depth': [5, 8, 15, 30, 80],\n",
    "    'max_leaf_nodes':[1, 3, 5, 10], \n",
    "    'max_samples': [1,3,5,10]\n",
    "    \n",
    "    #'max_features': [ 1, 3, 10],\n",
    "    #'n_estimators': [ 1, 5, 10]\n",
    "\n",
    "}\n",
    "\n",
    "# Set kflod parameters\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "#Set Gridsearch Parameters\n",
    "rf_grid = GridSearchCV(rf, param_grid = param_gs, cv = kf, n_jobs = -1, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7729d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Wall time: 20min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_jobs=-1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 8, 15, 30, 80],\n",
       "                         'max_leaf_nodes': [1, 3, 5, 10],\n",
       "                         'max_samples': [1, 3, 5, 10]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit model\n",
    "rf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f75e9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898343889436655\n",
      "0.8982528263103803\n"
     ]
    }
   ],
   "source": [
    "print(rf_grid.score(X_train, y_train))\n",
    "print(rf_grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "652fed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_leaf_nodes': 3, 'max_samples': 1}\n",
      "RandomForestClassifier(class_weight='balanced', max_depth=5, max_leaf_nodes=3,\n",
      "                       max_samples=1, n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(rf_grid.best_params_)\n",
    "print(rf_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49a34b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=5, max_leaf_nodes=3,\n",
       "                       max_samples=1, n_jobs=-1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fitting model with best params\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', max_depth=5, max_leaf_nodes=3,\n",
    "                       max_samples=1, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4cfc7549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Random Forrest Classifer.: 0.8982528263103803\n",
      "Precision : 0.8982528263103803\n",
      "Recall    : 0.802175908307403\n",
      "F1-score   : 0.8475001182536304\n",
      " train_score: 0.898343889436655\n",
      " test_score: 0.8982528263103803\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Random Forrest Classifer.: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "print(f' train_score: {rf.score(X_train, y_train)}')\n",
    "print(f' test_score: {rf.score(X_test, y_test)}')\n",
    "\n",
    "#Add performance parameters to list\n",
    "model_performance_tuned.append(dict([\n",
    "    ('Model', 'Random Forrest Classifier'),\n",
    "    ('Test Accuracy', accuracy),\n",
    "    ('Precision', precision),\n",
    "    ('Recall', recall),\n",
    "    ('F1', f1score),\n",
    "     ('Train Score', rf.score(X_train, y_train)),\n",
    "    ('Test Score', rf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e7f70",
   "metadata": {},
   "source": [
    "### GridSearch Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3e89b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re instantiate Logistic Regression\n",
    "lr = LogisticRegression() # we will insert the class weight = 'balance' later\n",
    "\n",
    "oneVsRes_lr1 = OneVsRestClassifier(lr) \n",
    "#oneVsRes_lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dd72eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['estimator__C', 'estimator__class_weight', 'estimator__dual', 'estimator__fit_intercept', 'estimator__intercept_scaling', 'estimator__l1_ratio', 'estimator__max_iter', 'estimator__multi_class', 'estimator__n_jobs', 'estimator__penalty', 'estimator__random_state', 'estimator__solver', 'estimator__tol', 'estimator__verbose', 'estimator__warm_start', 'estimator', 'n_jobs'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneVsRes_lr1.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ed5607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate parameters for Gridsearch\n",
    "lr_param_gs = {\n",
    "'estimator__class_weight' : ['balanced'],\n",
    "'estimator__penalty' : ['l1', 'l2'],\n",
    "'estimator__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    " 'estimator__C' : [100, 10, 1.0, 0.1, 0.01],\n",
    "    'n_jobs' : [-1]\n",
    "}\n",
    "\n",
    "\n",
    "# Set kfold parameters\n",
    "kf = KFold(n_splits = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "259a77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch\n",
    "lr_grid = GridSearchCV(oneVsRes_lr1, param_grid = lr_param_gs, cv = kf, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e166d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 46min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=OneVsRestClassifier(estimator=LogisticRegression()),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'estimator__C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                         'estimator__class_weight': ['balanced'],\n",
       "                         'estimator__penalty': ['l1', 'l2'],\n",
       "                         'estimator__solver': ['newton-cg', 'lbfgs', 'sag',\n",
       "                                               'saga'],\n",
       "                         'n_jobs': [-1]})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit model\n",
    "lr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1821ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856005265137086\n",
      "{'estimator__C': 0.01, 'estimator__class_weight': 'balanced', 'estimator__penalty': 'l2', 'estimator__solver': 'lbfgs', 'n_jobs': -1}\n",
      "OneVsRestClassifier(estimator=LogisticRegression(C=0.01,\n",
      "                                                 class_weight='balanced'),\n",
      "                    n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(lr_grid.best_score_)\n",
    "print(lr_grid.best_params_)\n",
    "print(lr_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a4ce080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model with best params\n",
    "\n",
    "lr = LogisticRegression(C = 0.01, class_weight='balanced', penalty= 'l2', solver= 'lbfgs',\n",
    "                        n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ce3ef6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=0.01,\n",
       "                                                 class_weight='balanced',\n",
       "                                                 n_jobs=-1))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model with best params\n",
    "oneVsRes_lr = OneVsRestClassifier(lr) \n",
    "oneVsRes_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b7a437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Logistic Regression: 0.8543102799989973 %\n",
      "Precision : 0.819183343440469\n",
      "Recall    : 0.9071881086163283\n",
      "F1-score   : 0.860942628610276\n",
      "Train_score : 0.8563144437574157\n",
      "Test_score : 0.8543102799989973\n"
     ]
    }
   ],
   "source": [
    "y_pred = oneVsRes_lr.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "accuracy, precision, recall, f1score = get_performance(y_test, y_pred)\n",
    "print(f'Test Accuracy Score of Logistic Regression: {accuracy}')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "print(f'Train_score : {oneVsRes_lr.score(X_train, y_train)}')\n",
    "print(f'Test_score : {oneVsRes_lr.score(X_test, y_test)}')\n",
    "\n",
    "# Add performance parameters to list\n",
    "model_performance_tuned.append(dict([\n",
    "    ('Model', 'Logistic Regression'),\n",
    "    ('Test Accuracy', accuracy),\n",
    "    ('Precision', precision),\n",
    "    ('Recall', recall),\n",
    "    ('F1', f1score),\n",
    "    ('Train Score', oneVsRes_lr.score(X_train, y_train)),\n",
    "    ('Test Score', oneVsRes_lr.score(X_test, y_test))\n",
    "    \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f27c9b",
   "metadata": {},
   "source": [
    "## 6. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f764468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.845838</td>\n",
       "      <td>0.820676</td>\n",
       "      <td>0.917329</td>\n",
       "      <td>0.866315</td>\n",
       "      <td>0.862740</td>\n",
       "      <td>0.845838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forrest Classifier</th>\n",
       "      <td>0.890231</td>\n",
       "      <td>0.864480</td>\n",
       "      <td>0.856081</td>\n",
       "      <td>0.860260</td>\n",
       "      <td>0.986322</td>\n",
       "      <td>0.890231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.763969</td>\n",
       "      <td>0.637818</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>0.763969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Test Accuracy  Precision    Recall        F1  \\\n",
       "Model                                                                     \n",
       "Logistic Regression             0.845838   0.820676  0.917329  0.866315   \n",
       "Random Forrest Classifier       0.890231   0.864480  0.856081  0.860260   \n",
       "Decision Tree Classifier        0.763969   0.637818  0.783797  0.703312   \n",
       "\n",
       "                           Train Score  Test Score  \n",
       "Model                                               \n",
       "Logistic Regression           0.862740    0.845838  \n",
       "Random Forrest Classifier     0.986322    0.890231  \n",
       "Decision Tree Classifier      0.985352    0.763969  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of the models without tunning (Baseline model scores)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f7243cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.854310</td>\n",
       "      <td>0.819183</td>\n",
       "      <td>0.907188</td>\n",
       "      <td>0.860943</td>\n",
       "      <td>0.856314</td>\n",
       "      <td>0.854310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forrest Classifier</th>\n",
       "      <td>0.898253</td>\n",
       "      <td>0.898253</td>\n",
       "      <td>0.802176</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.898344</td>\n",
       "      <td>0.898253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Test Accuracy  Precision    Recall        F1  \\\n",
       "Model                                                                     \n",
       "Logistic Regression             0.854310   0.819183  0.907188  0.860943   \n",
       "Random Forrest Classifier       0.898253   0.898253  0.802176  0.847500   \n",
       "\n",
       "                           Train Score  Test Score  \n",
       "Model                                               \n",
       "Logistic Regression           0.856314    0.854310  \n",
       "Random Forrest Classifier     0.898344    0.898253  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tuned score summary table\n",
    "results_tuned = pd.DataFrame(data= model_performance_tuned)\n",
    "results_tuned = results_tuned[[ 'Model','Test Accuracy', 'Precision', 'Recall', 'F1', 'Train Score', 'Test Score']]\n",
    "results_tuned = results_tuned.sort_values(by='F1', ascending=False)\n",
    "results_tuned = results_tuned.set_index('Model')\n",
    "results_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069676ab",
   "metadata": {},
   "source": [
    "- After hypertunning the parameters of the models, we have anaylsed that both model's F1 score was slightly reduced. however, overfitting were resolved. \n",
    "- With these observations, Logestic Regression performed the best amongst all thus it is the selected Model of Choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394cf7f",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937400d",
   "metadata": {},
   "source": [
    "From the modelling process,it is evident that Logistic Regression is the choice model to predict multi-label classification of the Toxic Words. \n",
    "It is also observed that top frequent words are co-related in Toxic, Severe_toxic, Insult and Obscene classifications. This relates that there is a co-relations in these classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d6f15",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eabd2a",
   "metadata": {},
   "source": [
    "Recently NLP Deep Learning models on text classification has been achieving state of the art results. It has shown that deep learning methods are able to provide for a suite of standard academic benchmark problems. Though we had achieve a considerably good modelling result here, we can explore utilising Deep learning models such as LSTM to predict the results.  Having establishing a a benchmark score here we can run deep learning models to analyse if they are indeed better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
